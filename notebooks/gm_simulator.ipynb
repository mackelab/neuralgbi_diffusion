{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "        from typing import List\n",
    "\n",
    "import torch\n",
    "from torch import tensor, Tensor, exp \n",
    "from torch.distributions import Distribution\n",
    "\n",
    "\n",
    "def ground_truth_mmd(\n",
    "    x: Tensor,\n",
    "    dists_y: List[Distribution],\n",
    "    y_limits: Tensor,\n",
    "    y_res: int = 100,\n",
    "    scale: float = 0.01,\n",
    "):\n",
    "    term1 = sample_based_mmd_marginal(x, x, scale=scale)\n",
    "    term2 = sample_integral_mixed_mmd_marginal(\n",
    "        x=x, y_dist=dists_y, y_limits=y_limits, y_res=y_res, scale=scale\n",
    "    )\n",
    "    term3 = integral_based_mmd_marginal(\n",
    "        x_dist=dists_y,\n",
    "        y_dist=dists_y,\n",
    "        x_limits=y_limits,\n",
    "        y_limits=y_limits,\n",
    "        x_res=y_res,\n",
    "        y_res=y_res,\n",
    "        scale=scale,\n",
    "    )\n",
    "    return term1 + term3 - 2 * term2\n",
    "\n",
    "\n",
    "def sample_based_mmd(x, y, scale: float = 0.01):\n",
    "    term1 = sample_based_mmd_marginal(x, x, scale=scale)\n",
    "    term2 = sample_based_mmd_marginal(x, y, scale=scale)\n",
    "    term3 = sample_based_mmd_marginal(y, y, scale=scale)\n",
    "    return term1 + term3 - 2 * term2\n",
    "\n",
    "\n",
    "def sample_based_mmd_marginal(x, y, scale: float = 0.01):\n",
    "    \"\"\"Assumes diagonal likelihood and sums over each dimension. Sum turns into\n",
    "    product because exp(sum) = prod(exp)\n",
    "    \"\"\"\n",
    "    dim = x.shape[1]\n",
    "    term = tensor(\n",
    "        [\n",
    "            sample_based_mmd_term(x[:, d : d + 1], y[:, d : d + 1], scale=scale)\n",
    "            for d in range(dim)\n",
    "        ]\n",
    "    ).prod()\n",
    "    return term\n",
    "\n",
    "\n",
    "def sample_based_mmd_term(x, y, scale: float = 0.01):\n",
    "    num_x = x.shape[0]\n",
    "    num_y = y.shape[0]\n",
    "    xo1 = x.repeat((num_y, 1))\n",
    "    xo2 = y.repeat_interleave((num_x), dim=0)\n",
    "    distances = exp(-scale * ((xo1 - xo2) ** 2).sum(dim=1))\n",
    "    average_dist = distances.mean(dim=0)\n",
    "    return average_dist\n",
    "\n",
    "\n",
    "def integral_based_mmd_marginal(\n",
    "    x_dist: List[Distribution],\n",
    "    y_dist: List[Distribution],\n",
    "    x_limits: Tensor,\n",
    "    y_limits: Tensor,\n",
    "    x_res: int = 100,\n",
    "    y_res: int = 100,\n",
    "    scale: float = 0.01,\n",
    "):\n",
    "    \"\"\"Assumes diagonal likelihood and sums over each dimension. Sum turns into\n",
    "    product because exp(sum) = prod(exp)\n",
    "    \"\"\"\n",
    "    dim = len(x_dist)\n",
    "    term = tensor(\n",
    "        [\n",
    "            integral_mmd_term(\n",
    "                x_dist[d],\n",
    "                y_dist[d],\n",
    "                x_limits[d],\n",
    "                y_limits[d],\n",
    "                x_res=x_res,\n",
    "                y_res=y_res,\n",
    "                scale=scale,\n",
    "            )\n",
    "            for d in range(dim)\n",
    "        ]\n",
    "    ).prod()\n",
    "    return term\n",
    "\n",
    "\n",
    "def integral_mmd_term(\n",
    "    x_dist: Distribution,\n",
    "    y_dist: Distribution,\n",
    "    x_limits: Tensor,\n",
    "    y_limits: Tensor,\n",
    "    x_res: int = 100,\n",
    "    y_res: int = 100,\n",
    "    scale: float = 0.01,\n",
    "):\n",
    "    x_range = torch.linspace(x_limits[0].item(), x_limits[1].item(), x_res).unsqueeze(1)\n",
    "    y_range = torch.linspace(y_limits[0].item(), y_limits[1].item(), y_res).unsqueeze(1)\n",
    "    x_repeat = x_range.repeat((y_res, 1))\n",
    "    y_repeat = y_range.repeat_interleave((x_res), dim=0)\n",
    "    probs_x = x_dist.log_prob(x_repeat).exp()\n",
    "    probs_y = y_dist.log_prob(y_repeat).exp()\n",
    "    distances = exp(-scale * ((x_repeat - y_repeat) ** 2).sum(dim=1))\n",
    "    dx = (x_limits[1].item() - x_limits[0].item()) / (x_res - 1)\n",
    "    dy = (y_limits[1].item() - y_limits[0].item()) / (y_res - 1)\n",
    "    integral = (probs_x * probs_y * distances).sum() * dx * dy\n",
    "    return integral\n",
    "\n",
    "\n",
    "def sample_integral_mixed_mmd_marginal(\n",
    "    x,\n",
    "    y_dist: List[Distribution],\n",
    "    y_limits: Tensor,\n",
    "    y_res: int = 100,\n",
    "    scale: float = 0.01,\n",
    "):\n",
    "    \"\"\"Assumes diagonal likelihood and sums over each dimension. Sum turns into\n",
    "    product because exp(sum) = prod(exp)\n",
    "    \"\"\"\n",
    "    dim = len(y_dist)\n",
    "    term = tensor(\n",
    "        [\n",
    "            sample_integral_mixed_mmd_term(\n",
    "                x[:, d : d + 1], y_dist[d], y_limits[d], y_res=y_res, scale=scale\n",
    "            )\n",
    "            for d in range(dim)\n",
    "        ]\n",
    "    ).prod()\n",
    "    return term\n",
    "\n",
    "\n",
    "def sample_integral_mixed_mmd_term(\n",
    "    x, y_dist: Distribution, y_limits: Tensor, y_res: int = 100, scale: float = 0.01\n",
    "):\n",
    "    y_range = torch.linspace(y_limits[0].item(), y_limits[1].item(), y_res).unsqueeze(1)\n",
    "    y_repeat = y_range.repeat((x.shape[0], 1))\n",
    "    probs_y = y_dist.log_prob(y_repeat).exp()\n",
    "    probs_y = torch.reshape(probs_y, (y_res, x.shape[0]))\n",
    "    y_reshape = torch.reshape(y_repeat, (y_res, x.shape[0], 1))\n",
    "    distances = exp(-scale * ((x - y_reshape) ** 2).sum(dim=2))\n",
    "    dy = (y_limits[1].item() - y_limits[0].item()) / (y_res - 1)\n",
    "    integrals = (distances * probs_y).sum(dim=0) * dy\n",
    "    monte_carlo_integral = torch.mean(integrals)\n",
    "    return monte_carlo_integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 115\u001b[0m\n\u001b[1;32m    113\u001b[0m theta \u001b[38;5;241m=\u001b[39m prior\u001b[38;5;241m.\u001b[39msample((\u001b[38;5;241m1000\u001b[39m,))\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m simulator\u001b[38;5;241m.\u001b[39msimulate(theta)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m--> 115\u001b[0m \u001b[43msimulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_misspecified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m    118\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[38;5;241m*\u001b[39mx[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn[16], line 60\u001b[0m, in \u001b[0;36mGaussianMixture.simulate_misspecified\u001b[0;34m(self, theta)\u001b[0m\n\u001b[1;32m     58\u001b[0m all_samples[\u001b[38;5;241m~\u001b[39mbern] \u001b[38;5;241m=\u001b[39m samples2[\u001b[38;5;241m~\u001b[39mbern]\n\u001b[1;32m     59\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mpermute(all_samples, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ((all_samples[:,:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimits[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m&\u001b[39m (all_samples[:,:,\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimits[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m&\u001b[39m (all_samples[:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m>\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimits[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m&\u001b[39m (all_samples[:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m<\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlimits[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mall()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_samples\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import tensor, ones, eye, Tensor\n",
    "from torch.distributions import MultivariateNormal, Distribution\n",
    "from sbi.utils import BoxUniform\n",
    "\n",
    "\n",
    "class GaussianMixture:\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_o: Optional[Tensor] = None,\n",
    "        num_trials: int = 5,\n",
    "        beta: float = 1.0,\n",
    "        dim: int = 2,\n",
    "        seed: int = 0,\n",
    "        limits: Tensor = tensor([[-14, 14], [-14, 14]]),\n",
    "        resolution: int = 250,\n",
    "        mmd_length_scale: float = 0.01,\n",
    "    ):\n",
    "        \"\"\"Suggested beta: [2.0, 10.0, 50.0]\"\"\"\n",
    "        # Set seed.\n",
    "        _ = torch.manual_seed(seed)\n",
    "        self.limits = limits\n",
    "        self.resolution = resolution\n",
    "        self.prior = BoxUniform(-10 * ones(dim), 10 * ones(dim))\n",
    "        self.x_o = x_o\n",
    "        # Ensure that shape is [5, 2], not [1, 5, 2].\n",
    "        if (self.x_o != None) and (len(self.x_o.shape) == 3):\n",
    "            raise ValueError(\"Gaussian mixture can not deal with batched observations.\")\n",
    "        self.num_trials = num_trials\n",
    "        self.beta = beta\n",
    "        self.mmd_length_scale = mmd_length_scale\n",
    "\n",
    "    def simulate(self, theta: Tensor) -> Tensor:\n",
    "        \"\"\"Simulator.\"\"\"\n",
    "        samples1 = torch.randn((self.num_trials, *theta.shape)) + theta\n",
    "        samples2 = 0.1 * torch.randn((self.num_trials, *theta.shape)) + theta\n",
    "        all_samples = torch.zeros(*samples1.shape)\n",
    "\n",
    "        bern = torch.bernoulli(0.5 * ones((self.num_trials, theta.shape[0]))).bool()\n",
    "\n",
    "        all_samples[bern] = samples1[bern]\n",
    "        all_samples[~bern] = samples2[~bern]\n",
    "        all_samples = torch.permute(all_samples, (1, 0, 2))\n",
    "        return all_samples\n",
    "\n",
    "    def simulate_misspecified(self, theta: Tensor) -> Tensor:\n",
    "        \"\"\"Simulator.\"\"\"\n",
    "        # For misspecified x, push it out of the prior bounds.\n",
    "        samples1 = torch.randn((self.num_trials, *theta.shape)) + theta\n",
    "        samples2 = 0.5 * torch.randn((self.num_trials, *theta.shape)) + torch.sign(theta)*12.5\n",
    "        all_samples = torch.zeros(*samples1.shape)\n",
    "\n",
    "        bern = torch.bernoulli(0.5 * ones((self.num_trials, theta.shape[0]))).bool()\n",
    "\n",
    "        all_samples[bern] = samples1[bern]\n",
    "        all_samples[~bern] = samples2[~bern]\n",
    "        all_samples = torch.permute(all_samples, (1, 0, 2))\n",
    "        assert ((all_samples[:,:,0]>self.limits[0,0]) & (all_samples[:,:,0]<self.limits[0,1]) & (all_samples[:,:,1]>self.limits[1,0]) & (all_samples[:,:,1]<self.limits[1,1])).all()\n",
    "        return all_samples\n",
    "        # samples = 0.5 * torch.randn((self.num_trials, *theta.shape)) + theta\n",
    "        # samples = torch.permute(samples, (1, 0, 2))\n",
    "        # return samples\n",
    "\n",
    "        \n",
    "\n",
    "    def build_marginal_dist(self, predicted_mean):\n",
    "        class MixtureDist(Distribution):\n",
    "            def __init__(self, predicted_mean):\n",
    "                super().__init__()\n",
    "                self.dist1 = MultivariateNormal(tensor([predicted_mean]), eye(1))\n",
    "                self.dist2 = MultivariateNormal(tensor([predicted_mean]), 0.01 * eye(1))\n",
    "\n",
    "            def log_prob(self, x):\n",
    "                prob1 = self.dist1.log_prob(x).exp()\n",
    "                prob2 = self.dist1.log_prob(x).exp()\n",
    "                return (0.5 * prob1 + 0.5 * prob2).log()\n",
    "\n",
    "        marginals = [MixtureDist(p) for p in predicted_mean[0]]\n",
    "        return marginals\n",
    "\n",
    "    def distance_fn(self, theta):\n",
    "        \"\"\"Computes E_{x|t}[(x - x_o)^2].\"\"\"\n",
    "        assert self.x_o is not None, \"x_o not set.\"\n",
    "        if theta.ndim == 1:\n",
    "            theta = theta.unsqueeze(0)\n",
    "\n",
    "        marginals = self.build_marginal_dist(theta)\n",
    "        mmd_x = ground_truth_mmd(\n",
    "            x=self.x_o,\n",
    "            dists_y=marginals,\n",
    "            y_limits=self.limits,\n",
    "            y_res=self.resolution,\n",
    "            scale=self.mmd_length_scale,\n",
    "        )\n",
    "        return mmd_x\n",
    "\n",
    "    def potential(self, theta):\n",
    "        \"\"\"Potential for GBI ground truth posterior.\"\"\"\n",
    "        if theta.ndim == 1:\n",
    "            theta = theta.unsqueeze(0)\n",
    "\n",
    "        potentials = []\n",
    "        for t in theta:\n",
    "            term1 = -self.beta * self.distance_fn(t)\n",
    "            potentials.append(term1 + self.prior.log_prob(t))\n",
    "        return torch.stack(potentials)\n",
    "    \n",
    "\n",
    "simulator = GaussianMixture()\n",
    "prior = simulator.prior\n",
    "theta = prior.sample((1000,))\n",
    "x = simulator.simulate(theta).numpy()\n",
    "simulator.simulate_misspecified(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
